# GEMINI.MD: AI Collaboration Guide

This document provides essential context for AI models interacting with this project. Adhering to these guidelines will ensure consistency and maintain code quality.

## 1. User-Provided Instructions & Overrides

_This section is for human developers to provide explicit instructions, overrides, or clarifications that might not be obvious from the codebase. AI collaborators should treat this section as the highest source of truth and follow its directives without exception._

[//]: # (USER_SECTION_START)
# Directive: New Documentation Source Integration Workflow

## 1. Overview

This document defines the mandatory, semi-automated procedure for integrating a new documentation source into the `llms-docs-ext` extension. You are to act as the agent executing this workflow upon receiving the specified trigger prompt from the user.

## 2. Trigger Prompt

This workflow must be initiated when the user provides a prompt in the following format:

> "Add a new documentation source: `[Name of Source]` from `[URL to llms.txt]`"

## 3. Integration Procedure

Upon receiving the trigger prompt, you must execute the following steps precisely.

### Step 3.1: Fetch and Analyze Primary Source

1.  Use the `web_fetch` tool to retrieve the content of the `[URL to llms.txt]` provided by the user.
2.  From the fetched content, parse and list all unique second-level URLs (those ending in `.md.txt` or similar markdown formats).
3.  Use the `web_fetch` tool again on a representative sample of these second-level URLs (the first 3-5 unique URLs) to perform a deeper analysis of the actual documentation content.

### Step 3.2: Synthesize Capabilities

1.  Based on your deep analysis, create a concise, bulleted list of the key **capabilities** and **high-level concepts** covered by this documentation. 
2.  Do not list individual functions or methods. Focus on logical groupings (e.g., "Authentication", "Content Generation", "Data Handling").
3.  Present this summary to the user for confirmation by saying: "I have analyzed the source. It appears to cover the following topics: [Bulleted List]. Shall I proceed with integrating it?"

### Step 3.3: Update Extension Configuration

1.  **Only after user confirmation**, read the contents of the `/llms-docs-ext/config.json` file.
2.  Append a new JSON object to the array with the `"name": "[Name of Source]"` and `"llms_txt": "[URL to llms.txt]"`.
3.  Write the updated content back to the `/llms-docs-ext/config.json` file.

### Step 3.4: Update Extension `GEMINI.md`

1.  Read the contents of the `/llms-docs-ext/GEMINI.md` file.
2.  Append a new entry to the "Activation Hooks & Supported Topics" section.
3.  This entry must be in the following format:

    ```markdown
    *   **Topic: [Name of Source]**
        *   **Hook:** Any question, code request, or query about using the [Name of Source].
        *   **Covered Concepts:** [Bulleted list of capabilities synthesized in Step 3.2].
        *   **Example:** "[Create a relevant example query based on the covered concepts]."
    ```
4.  Write the updated content back to the `/llms-docs-ext/GEMINI.md` file.

### Step 3.5: Final Confirmation

1.  Report the successful integration to the user by stating: "I have successfully integrated the `[Name of Source]` documentation into the extension."
[//]: # (USER_SECTION_END)

## 2. AI-Generated Project Analysis

_This section is automatically generated by analyzing the project structure. It should be used as a reference for the project's conventions and architecture but is superseded by the user-provided instructions above._

### 2.1. Project Overview & Purpose

*   **Primary Goal:** This project is a documentation-aware extension for the Gemini CLI. Its purpose is to ensure the CLI can answer questions about various libraries and frameworks using the latest, official documentation, rather than relying solely on its internal knowledge.
*   **Business Domain:** AI-assisted Software Development, Developer Tools.

### 2.2. Core Technologies & Stack

*   **Languages:** JSON for configuration. The underlying MCP server is invoked via `uvx`, indicating a Python environment.
*   **Frameworks & Runtimes:** Gemini CLI Extension, Model Context Protocol (MCP).
*   **Databases:** None detected.
*   **Key Libraries/Dependencies:** `mcpdoc` (inferred from `gemini-extension.json`).
*   **Package Manager(s):** `uv` (inferred from the `uvx` command).

### 2.3. Architectural Patterns

*   **Overall Architecture:** This project follows a CLI Extension pattern. It operates on a client-server model where the extension, running within the Gemini CLI, communicates with a local MCP server to fetch documentation context.
*   **Directory Structure Philosophy:** 
    *   `/`: The root directory contains project-level documentation (`README.md`), license, and the primary AI collaboration guide (`GEMINI.md`).
    *   `/llms-docs-mcp`: This directory contains the core extension definition (`gemini-extension.json`) and the specific AI context (`GEMINI.md`) that instructs the model on how to use the provided documentation.

### 2.4. Coding Conventions & Style Guide

*   **Formatting:** JSON files follow standard formatting with a 2-space indentation.
*   **Naming Conventions:** 
    *   `files`: kebab-case (`gemini-extension.json`) or `UPPERCASE` (`README.md`).
    *   `JSON keys`: camelCase (`mcpServers`, `contextFileName`).
*   **API Design:** Not a traditional web API. Interaction is defined by the Gemini CLI extension system and the Model Context Protocol (MCP) for data exchange.
*   **Error Handling:** No explicit error handling patterns are observable from the project's configuration files.

### 2.5. Key Files & Entrypoints

*   **Main Entrypoint(s):** The Gemini CLI loads the extension through the `llms-docs-mcp/gemini-extension.json` file. The `command` and `args` within this file define the startup process for the MCP server.
*   **Configuration:** 
    *   `llms-docs-mcp/gemini-extension.json`: The primary configuration file that defines the extension's name, version, and the MCP server command.
    *   `GEMINI.md` (root): Contains high-level directives for the AI on how to interact with and manage the extension.
    *   `llms-docs-mcp/GEMINI.md`: Provides specific instructions to the AI on how to use the documentation context provided by the MCP server.
*   **CI/CD Pipeline:** No CI/CD pipeline (e.g., `.github/workflows`) was detected.

### 2.6. Development & Testing Workflow

*   **Local Development Environment:** As described in the `README.md`, the project is set up by cloning the repository into the Gemini CLI's extensions directory (`~/.gemini/extensions` or `./.gemini/extensions`).
*   **Testing:** The `README.md` states that there is currently no dedicated test suite for this project. Verification is done by running queries against the CLI.
*   **CI/CD Process:** No CI/CD process was detected.

### 2.7. Specific Instructions for AI Collaboration

*   **Contribution Guidelines:** Contributions are encouraged. The standard workflow is to fork the repository, create a feature branch, make changes, and submit a pull request.
*   **Infrastructure (IaC):** No Infrastructure as Code (IaC) files were detected.
*   **Security:** Be mindful of security best practices. Do not hardcode secrets or keys.
*   **Dependencies:** New documentation sources are not added by modifying package files but by using the specific prompt-based workflow defined in the User-Provided Instructions section of this document.
*   **Commit Messages:** No specific commit message format is enforced. It is recommended to follow Conventional Commits style (e.g., `feat:`, `fix:`, `docs:`) for clarity.